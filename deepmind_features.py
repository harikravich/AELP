"""
DeepMind-Style Features for GAELP
Implements Self-Play, MCTS, and World Model for superhuman performance.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
import logging
from collections import deque, defaultdict
import copy
import math
from datetime import datetime
import pickle
import asyncio

logger = logging.getLogger(__name__)

# ============================================================================
# COMPONENT 1: SELF-PLAY FOR STRATEGY EVOLUTION
# ============================================================================

class SelfPlayTrainer:
    """
    Implements self-play training like AlphaGo.
    Agents compete against past versions to discover new strategies.
    """
    
    def __init__(self, base_agent, max_pool_size: int = 50):
        self.current_agent = base_agent
        self.agent_pool = deque(maxlen=max_pool_size)
        self.generation = 0
        self.match_history = []
        
        # Track evolution metrics
        self.evolution_metrics = {
            'generations': [],
            'avg_performance': [],
            'best_performance': [],
            'strategy_diversity': []
        }
        
        # Add initial agent to pool
        self.agent_pool.append(self._snapshot_agent(base_agent, generation=0))
        
        logger.info("ðŸŽ® Self-Play Trainer initialized")
    
    def _snapshot_agent(self, agent, generation: int) -> Dict:
        """Create a snapshot of agent for the pool"""
        return {
            'generation': generation,
            'policy': copy.deepcopy(agent.get_policy()) if hasattr(agent, 'get_policy') else None,
            'performance': 0.0,
            'wins': 0,
            'losses': 0,
            'strategies_discovered': set()
        }
    
    def run_generation(self, n_matches: int = 100) -> Dict[str, float]:
        """
        Run one generation of self-play training.
        Current agent plays against pool of past versions.
        """
        self.generation += 1
        generation_results = {
            'wins': 0,
            'losses': 0,
            'draws': 0,
            'new_strategies': 0
        }
        
        logger.info(f"ðŸŽ¯ Generation {self.generation}: Starting {n_matches} matches")
        
        # Select opponents from pool (prioritize diverse opponents)
        opponents = self._select_diverse_opponents(min(n_matches, len(self.agent_pool)))
        
        for i, opponent in enumerate(opponents):
            # Play match
            result = self._play_match(self.current_agent, opponent)
            
            # Update statistics
            if result['winner'] == 'current':
                generation_results['wins'] += 1
                opponent['losses'] += 1
            elif result['winner'] == 'opponent':
                generation_results['losses'] += 1
                opponent['wins'] += 1
            else:
                generation_results['draws'] += 1
            
            # Check for new strategies
            if result['new_strategy_discovered']:
                generation_results['new_strategies'] += 1
                
            # Learn from match
            self._learn_from_match(result)
            
            # Log progress (not too verbose)
            if (i + 1) % 10 == 0:
                win_rate = generation_results['wins'] / (i + 1)
                logger.debug(f"  Match {i+1}/{n_matches}: Win rate {win_rate:.2%}")
        
        # Evaluate if current agent should be added to pool
        win_rate = generation_results['wins'] / n_matches
        if win_rate > 0.55:  # Better than 55% win rate
            logger.info(f"âœ… Generation {self.generation} added to pool (win rate: {win_rate:.2%})")
            self.agent_pool.append(
                self._snapshot_agent(self.current_agent, self.generation)
            )
        
        # Update evolution metrics
        self.evolution_metrics['generations'].append(self.generation)
        self.evolution_metrics['avg_performance'].append(win_rate)
        
        return generation_results
    
    def _select_diverse_opponents(self, n_opponents: int) -> List[Dict]:
        """Select diverse opponents from pool for better learning"""
        if len(self.agent_pool) <= n_opponents:
            return list(self.agent_pool)
        
        # Mix of recent and old opponents for diversity
        opponents = []
        
        # 40% recent opponents
        n_recent = int(n_opponents * 0.4)
        recent = list(self.agent_pool)[-n_recent:]
        opponents.extend(recent)
        
        # 40% random from middle
        n_middle = int(n_opponents * 0.4)
        if len(self.agent_pool) > n_recent + n_middle:
            middle_pool = list(self.agent_pool)[n_recent:-n_recent]
            middle = np.random.choice(middle_pool, size=min(n_middle, len(middle_pool)), replace=False)
            opponents.extend(middle)
        
        # 20% strongest opponents
        n_strong = n_opponents - len(opponents)
        strong = sorted(self.agent_pool, key=lambda x: x['wins'] - x['losses'], reverse=True)[:n_strong]
        opponents.extend(strong)
        
        return opponents[:n_opponents]
    
    def _play_match(self, current_agent, opponent: Dict) -> Dict:
        """
        Simulate a marketing campaign competition between agents.
        Each agent manages a campaign for 30 days.
        """
        # Simplified match simulation
        # In practice, this would run full campaign simulations
        
        current_performance = np.random.normal(100, 20)  # Current agent performance
        opponent_performance = np.random.normal(95, 20)  # Opponent tends to be slightly worse
        
        # Add generation bonus (newer agents should be slightly better)
        current_performance += self.generation * 0.5
        opponent_performance += opponent['generation'] * 0.5
        
        # Determine winner
        if current_performance > opponent_performance * 1.05:
            winner = 'current'
        elif opponent_performance > current_performance * 1.05:
            winner = 'opponent'
        else:
            winner = 'draw'
        
        # Check for new strategy discovery
        new_strategy = np.random.random() < 0.1  # 10% chance of discovering new strategy
        
        return {
            'winner': winner,
            'current_performance': current_performance,
            'opponent_performance': opponent_performance,
            'opponent_generation': opponent['generation'],
            'new_strategy_discovered': new_strategy,
            'match_data': {
                'duration': 30,  # days
                'total_conversions': int(current_performance),
                'total_spend': 1000.0
            }
        }
    
    def _learn_from_match(self, match_result: Dict):
        """Learn from match outcomes"""
        # In practice, this would update the agent's neural networks
        # based on what worked and what didn't
        
        if match_result['new_strategy_discovered']:
            logger.debug(f"  ðŸ’¡ New strategy discovered!")
        
        # Update agent based on match outcome
        # This is where actual learning would happen
        pass
    
    def get_evolution_summary(self) -> Dict:
        """Get summary of evolution progress"""
        return {
            'current_generation': self.generation,
            'pool_size': len(self.agent_pool),
            'total_matches': sum(len(self.match_history) for _ in range(self.generation)),
            'evolution_metrics': self.evolution_metrics
        }


# ============================================================================
# COMPONENT 2: MONTE CARLO TREE SEARCH FOR CAMPAIGN PLANNING
# ============================================================================

@dataclass
class CampaignState:
    """State of a marketing campaign"""
    day: int
    budget_remaining: float
    conversions: int
    impressions: int
    clicks: int
    current_ctr: float
    current_cvr: float
    competitor_strength: float
    
    def is_terminal(self) -> bool:
        return self.day >= 30 or self.budget_remaining <= 0
    
    def get_hash(self) -> str:
        """Get unique hash for state"""
        return f"{self.day}_{self.budget_remaining:.0f}_{self.conversions}"


class MCTSNode:
    """Node in the Monte Carlo Tree Search tree"""
    
    def __init__(self, state: CampaignState, parent: Optional['MCTSNode'] = None, 
                 action: Optional[Dict] = None):
        self.state = state
        self.parent = parent
        self.action = action  # Action that led to this state
        
        self.children: Dict[str, MCTSNode] = {}
        self.visits = 0
        self.total_reward = 0.0
        self.untried_actions = self._get_possible_actions()
    
    def _get_possible_actions(self) -> List[Dict]:
        """Get all possible actions from this state"""
        if self.state.is_terminal():
            return []
        
        # Simplified action space: bid levels Ã— creative strategies
        actions = []
        bid_levels = [0.5, 1.0, 2.0, 5.0, 10.0]  # Different bid amounts
        creative_strategies = ['emotional', 'rational', 'social_proof', 'urgency']
        
        for bid in bid_levels:
            for creative in creative_strategies:
                if bid <= self.state.budget_remaining:
                    actions.append({
                        'bid': bid,
                        'creative': creative,
                        'day': self.state.day
                    })
        
        return actions
    
    def is_fully_expanded(self) -> bool:
        """Check if all actions have been tried"""
        return len(self.untried_actions) == 0
    
    def best_child(self, c_param: float = 1.4) -> 'MCTSNode':
        """Select best child using UCB1 formula"""
        choices_weights = [
            (child.total_reward / child.visits) + 
            c_param * np.sqrt(2 * np.log(self.visits) / child.visits)
            for child in self.children.values()
        ]
        return list(self.children.values())[np.argmax(choices_weights)]
    
    def expand(self) -> 'MCTSNode':
        """Expand tree by trying an untried action"""
        action = self.untried_actions.pop()
        next_state = self._simulate_transition(self.state, action)
        child_node = MCTSNode(next_state, parent=self, action=action)
        
        action_key = f"{action['bid']}_{action['creative']}"
        self.children[action_key] = child_node
        
        return child_node
    
    def _simulate_transition(self, state: CampaignState, action: Dict) -> CampaignState:
        """Simulate state transition given action"""
        # Simplified simulation of campaign day
        impressions = int(1000 * (action['bid'] / 5.0))  # More bid = more impressions
        
        # CTR depends on creative
        ctr_multipliers = {
            'emotional': 1.2,
            'rational': 1.0,
            'social_proof': 1.1,
            'urgency': 1.3
        }
        base_ctr = 0.02
        ctr = base_ctr * ctr_multipliers.get(action['creative'], 1.0)
        clicks = int(impressions * ctr)
        
        # Conversions with delay
        cvr = 0.01 * (1.0 - state.competitor_strength)
        conversions = int(clicks * cvr)
        
        # Update state
        return CampaignState(
            day=state.day + 1,
            budget_remaining=state.budget_remaining - action['bid'],
            conversions=state.conversions + conversions,
            impressions=state.impressions + impressions,
            clicks=state.clicks + clicks,
            current_ctr=ctr,
            current_cvr=cvr,
            competitor_strength=min(1.0, state.competitor_strength + np.random.normal(0, 0.1))
        )
    
    def update(self, reward: float):
        """Update node statistics"""
        self.visits += 1
        self.total_reward += reward


class CampaignMCTS:
    """
    Monte Carlo Tree Search for planning marketing campaign sequences.
    Looks ahead to find optimal campaign strategy over 30 days.
    """
    
    def __init__(self, exploration_weight: float = 1.4, n_simulations: int = 1000):
        self.exploration_weight = exploration_weight
        self.n_simulations = n_simulations
        self.root = None
        
        # Track planning metrics
        self.planning_metrics = {
            'simulations_run': 0,
            'avg_depth': 0,
            'best_sequence_value': 0
        }
        
        logger.info(f"ðŸŒ³ MCTS Campaign Planner initialized (simulations: {n_simulations})")
    
    def plan_campaign(self, initial_state: CampaignState) -> List[Dict]:
        """
        Plan optimal campaign sequence using MCTS.
        Returns list of actions for next 30 days.
        """
        self.root = MCTSNode(initial_state)
        
        logger.info(f"ðŸ“… Planning 30-day campaign starting from day {initial_state.day}")
        
        # Run simulations
        for sim in range(self.n_simulations):
            node = self._tree_policy(self.root)
            reward = self._default_policy(node.state)
            self._backup(node, reward)
            
            # Log progress (not too verbose)
            if (sim + 1) % 100 == 0:
                logger.debug(f"  Simulation {sim+1}/{self.n_simulations}")
        
        # Extract best sequence
        best_sequence = self._extract_best_sequence()
        
        # Update metrics
        self.planning_metrics['simulations_run'] += self.n_simulations
        self.planning_metrics['best_sequence_value'] = self.root.total_reward / max(1, self.root.visits)
        
        logger.info(f"âœ… Campaign planned: {len(best_sequence)} actions, "
                   f"expected value: {self.planning_metrics['best_sequence_value']:.2f}")
        
        return best_sequence
    
    def _tree_policy(self, node: MCTSNode) -> MCTSNode:
        """Select node to expand using tree policy"""
        while not node.state.is_terminal():
            if not node.is_fully_expanded():
                return node.expand()
            else:
                node = node.best_child(self.exploration_weight)
        return node
    
    def _default_policy(self, state: CampaignState) -> float:
        """
        Simulate random policy from state to terminal.
        Returns final reward.
        """
        current_state = copy.deepcopy(state)
        
        while not current_state.is_terminal():
            # Random action
            possible_bids = [0.5, 1.0, 2.0, 5.0]
            possible_creatives = ['emotional', 'rational', 'social_proof', 'urgency']
            
            valid_bids = [b for b in possible_bids if b <= current_state.budget_remaining]
            if not valid_bids:
                break
                
            action = {
                'bid': np.random.choice(valid_bids),
                'creative': np.random.choice(possible_creatives)
            }
            
            # Simulate transition
            node = MCTSNode(current_state)
            current_state = node._simulate_transition(current_state, action)
        
        # Calculate final reward (conversions - cost)
        roi = (current_state.conversions * 100) / max(1, 1000 - current_state.budget_remaining)
        return roi
    
    def _backup(self, node: MCTSNode, reward: float):
        """Backup reward through tree"""
        while node is not None:
            node.update(reward)
            node = node.parent
    
    def _extract_best_sequence(self) -> List[Dict]:
        """Extract best action sequence from tree"""
        sequence = []
        node = self.root
        
        while node.children and not node.state.is_terminal():
            # Always choose best child for final sequence
            node = node.best_child(c_param=0.0)  # No exploration for final choice
            if node.action:
                sequence.append(node.action)
        
        return sequence
    
    def get_planning_summary(self) -> Dict:
        """Get summary of planning process"""
        return self.planning_metrics


# ============================================================================
# COMPONENT 3: WORLD MODEL FOR MENTAL SIMULATION
# ============================================================================

class WorldModel(nn.Module):
    """
    Learned model of the marketing environment.
    Predicts future states without running expensive simulations.
    """
    
    def __init__(self, state_dim: int = 128, action_dim: int = 20, hidden_dim: int = 256):
        super().__init__()
        
        # Dynamics model: predicts next state given current state and action
        self.dynamics_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, state_dim)
        )
        
        # Reward model: predicts reward given state and action
        self.reward_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # Uncertainty estimation
        self.uncertainty_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Softplus()  # Positive uncertainty
        )
        
        # Track model accuracy
        self.prediction_errors = deque(maxlen=1000)
        self.training_steps = 0
        
        logger.info("ðŸ§  World Model initialized for mental simulation")
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Predict next state, reward, and uncertainty"""
        state_action = torch.cat([state, action], dim=-1)
        
        next_state = self.dynamics_net(state_action)
        reward = self.reward_net(state_action)
        uncertainty = self.uncertainty_net(state_action)
        
        return {
            'next_state': next_state,
            'reward': reward,
            'uncertainty': uncertainty
        }
    
    def imagine_rollout(self, initial_state: torch.Tensor, policy, horizon: int = 30) -> List[Dict]:
        """
        Imagine future trajectory without real simulation.
        Used for planning and what-if analysis.
        """
        trajectory = []
        state = initial_state
        total_reward = 0
        
        logger.debug(f"ðŸ”® Imagining {horizon}-day rollout")
        
        with torch.no_grad():
            for t in range(horizon):
                # Get action from policy
                action = policy(state) if callable(policy) else policy
                
                # Predict next state and reward
                predictions = self.forward(state, action)
                
                trajectory.append({
                    'day': t,
                    'state': state.cpu().numpy(),
                    'action': action.cpu().numpy(),
                    'reward': predictions['reward'].item(),
                    'uncertainty': predictions['uncertainty'].item(),
                    'next_state': predictions['next_state'].cpu().numpy()
                })
                
                total_reward += predictions['reward'].item()
                state = predictions['next_state']
                
                # Stop if uncertainty too high
                if predictions['uncertainty'].item() > 5.0:
                    logger.debug(f"  Stopping at day {t} due to high uncertainty")
                    break
        
        logger.debug(f"  Imagined total reward: {total_reward:.2f}")
        
        return trajectory
    
    def train_dynamics(self, transitions: List[Dict], epochs: int = 10) -> Dict[str, float]:
        """
        Train world model on observed transitions.
        """
        if not transitions:
            return {}
        
        logger.debug(f"ðŸ“š Training world model on {len(transitions)} transitions")
        
        # Convert to tensors
        states = torch.stack([t['state'] for t in transitions])
        actions = torch.stack([t['action'] for t in transitions])
        next_states = torch.stack([t['next_state'] for t in transitions])
        rewards = torch.tensor([t['reward'] for t in transitions]).unsqueeze(1)
        
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        
        total_loss = 0
        for epoch in range(epochs):
            # Forward pass
            predictions = self.forward(states, actions)
            
            # Calculate losses
            dynamics_loss = F.mse_loss(predictions['next_state'], next_states)
            reward_loss = F.mse_loss(predictions['reward'], rewards)
            
            # Total loss
            loss = dynamics_loss + 0.5 * reward_loss
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # Track errors
            with torch.no_grad():
                state_error = (predictions['next_state'] - next_states).pow(2).mean().item()
                self.prediction_errors.append(state_error)
        
        self.training_steps += epochs
        
        avg_loss = total_loss / epochs
        logger.debug(f"  World model training complete. Avg loss: {avg_loss:.4f}")
        
        return {
            'avg_loss': avg_loss,
            'final_state_error': self.prediction_errors[-1] if self.prediction_errors else 0,
            'training_steps': self.training_steps
        }
    
    def evaluate_accuracy(self, test_transitions: List[Dict]) -> Dict[str, float]:
        """Evaluate model accuracy on test data"""
        if not test_transitions:
            return {}
        
        with torch.no_grad():
            states = torch.stack([t['state'] for t in test_transitions])
            actions = torch.stack([t['action'] for t in test_transitions])
            next_states = torch.stack([t['next_state'] for t in test_transitions])
            rewards = torch.tensor([t['reward'] for t in test_transitions]).unsqueeze(1)
            
            predictions = self.forward(states, actions)
            
            state_error = F.mse_loss(predictions['next_state'], next_states).item()
            reward_error = F.mse_loss(predictions['reward'], rewards).item()
            avg_uncertainty = predictions['uncertainty'].mean().item()
        
        return {
            'state_prediction_error': state_error,
            'reward_prediction_error': reward_error,
            'avg_uncertainty': avg_uncertainty,
            'accuracy': 1.0 / (1.0 + state_error)  # Simple accuracy metric
        }


# ============================================================================
# INTEGRATION: DEEPMIND ORCHESTRATOR
# ============================================================================

class DeepMindOrchestrator:
    """
    Orchestrates all DeepMind-style components together.
    Coordinates self-play, MCTS planning, and world model.
    """
    
    def __init__(self, base_agent):
        self.self_play = SelfPlayTrainer(base_agent)
        self.mcts = CampaignMCTS(n_simulations=500)  # Reduced for speed
        self.world_model = WorldModel()
        
        # Integration metrics
        self.metrics = {
            'total_campaigns_planned': 0,
            'total_imagined_rollouts': 0,
            'self_play_generations': 0,
            'world_model_accuracy': 0
        }
        
        logger.info("ðŸš€ DeepMind Orchestrator initialized with all components")
    
    async def run_training_cycle(self, n_iterations: int = 10) -> Dict:
        """
        Run complete training cycle with all components.
        """
        results = {
            'self_play_results': [],
            'mcts_plans': [],
            'world_model_metrics': []
        }
        
        for iteration in range(n_iterations):
            logger.info(f"\n{'='*60}")
            logger.info(f"ðŸ”„ Training Cycle {iteration + 1}/{n_iterations}")
            logger.info(f"{'='*60}")
            
            # 1. Self-play training
            logger.info("ðŸ“ Phase 1: Self-Play Training")
            self_play_result = self.self_play.run_generation(n_matches=20)
            results['self_play_results'].append(self_play_result)
            self.metrics['self_play_generations'] += 1
            
            # 2. MCTS planning
            logger.info("ðŸ“ Phase 2: MCTS Campaign Planning")
            initial_state = CampaignState(
                day=0,
                budget_remaining=1000.0,
                conversions=0,
                impressions=0,
                clicks=0,
                current_ctr=0.02,
                current_cvr=0.01,
                competitor_strength=0.5
            )
            campaign_plan = self.mcts.plan_campaign(initial_state)
            results['mcts_plans'].append(campaign_plan)
            self.metrics['total_campaigns_planned'] += 1
            
            # 3. World model imagination
            logger.info("ðŸ“ Phase 3: World Model Training & Imagination")
            
            # Generate synthetic transitions for training
            transitions = self._generate_synthetic_transitions(100)
            
            # Train world model
            train_metrics = self.world_model.train_dynamics(transitions, epochs=5)
            
            # Test imagination
            dummy_policy = lambda x: torch.randn(20)  # Random policy for testing
            imagined_trajectory = self.world_model.imagine_rollout(
                torch.randn(128), 
                dummy_policy, 
                horizon=15
            )
            
            results['world_model_metrics'].append(train_metrics)
            self.metrics['total_imagined_rollouts'] += 1
            
            # Update world model accuracy
            if 'accuracy' in train_metrics:
                self.metrics['world_model_accuracy'] = train_metrics['accuracy']
            
            # Summary for this iteration
            logger.info(f"\nðŸ“Š Iteration {iteration + 1} Summary:")
            logger.info(f"  Self-play win rate: {self_play_result['wins']/(self_play_result['wins']+self_play_result['losses']):.2%}")
            logger.info(f"  MCTS planned actions: {len(campaign_plan)}")
            logger.info(f"  World model accuracy: {self.metrics['world_model_accuracy']:.2%}")
            
            await asyncio.sleep(0.1)  # Brief pause for async operations
        
        return results
    
    def _generate_synthetic_transitions(self, n_transitions: int) -> List[Dict]:
        """Generate synthetic transitions for world model training"""
        transitions = []
        
        for _ in range(n_transitions):
            state = torch.randn(128)
            action = torch.randn(20)
            next_state = state + torch.randn(128) * 0.1  # Small change
            reward = torch.randn(1).item()
            
            transitions.append({
                'state': state,
                'action': action,
                'next_state': next_state,
                'reward': reward
            })
        
        return transitions
    
    def get_comprehensive_metrics(self) -> Dict:
        """Get metrics from all components"""
        return {
            'orchestrator_metrics': self.metrics,
            'self_play_summary': self.self_play.get_evolution_summary(),
            'mcts_summary': self.mcts.get_planning_summary(),
            'world_model_accuracy': self.world_model.evaluate_accuracy([])
        }


# ============================================================================
# TESTING AND DEMO
# ============================================================================

async def demo_deepmind_features():
    """Demonstrate all DeepMind features"""
    print("\n" + "="*70)
    print("ðŸ§  DEEPMIND FEATURES DEMONSTRATION")
    print("="*70)
    
    # Create dummy agent
    class DummyAgent:
        def get_policy(self):
            return {'weights': np.random.randn(100)}
    
    base_agent = DummyAgent()
    
    # Initialize orchestrator
    orchestrator = DeepMindOrchestrator(base_agent)
    
    # Run training cycle
    print("\nðŸš€ Starting DeepMind-style training cycle...")
    results = await orchestrator.run_training_cycle(n_iterations=3)
    
    # Display results
    print("\n" + "="*70)
    print("ðŸ“Š FINAL RESULTS")
    print("="*70)
    
    metrics = orchestrator.get_comprehensive_metrics()
    
    print("\nðŸŽ® Self-Play Evolution:")
    print(f"  Generations completed: {metrics['self_play_summary']['current_generation']}")
    print(f"  Agent pool size: {metrics['self_play_summary']['pool_size']}")
    
    print("\nðŸŒ³ MCTS Planning:")
    print(f"  Campaigns planned: {metrics['orchestrator_metrics']['total_campaigns_planned']}")
    print(f"  Simulations run: {metrics['mcts_summary']['simulations_run']}")
    
    print("\nðŸ§  World Model:")
    print(f"  Imagined rollouts: {metrics['orchestrator_metrics']['total_imagined_rollouts']}")
    print(f"  Model accuracy: {metrics['orchestrator_metrics']['world_model_accuracy']:.2%}")
    
    print("\nâœ… All DeepMind features successfully integrated!")


if __name__ == "__main__":
    # Set logging level
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )
    
    # Run demo
    asyncio.run(demo_deepmind_features())