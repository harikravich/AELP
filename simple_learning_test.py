#!/usr/bin/env python3
"""
Simple Learning Verification Test

This script performs basic verification that learning concepts work
without complex dependencies. Tests fundamental learning mechanics.

CRITICAL TESTS:
1. Basic neural network gradient flow
2. Optimizer weight updates  
3. Loss improvement over iterations
4. Entropy changes in policy networks
5. Memory and gradient accumulation

NO DEPENDENCIES ON COMPLEX GAELP COMPONENTS.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimplePolicyNetwork(nn.Module):
    """Simple policy network for testing"""
    
    def __init__(self, state_dim: int = 64, action_dim: int = 8):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)
    
    def sample_action(self, state):
        """Sample action with log probability"""
        logits = self.forward(state)
        probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(probs, 1)
        log_prob = torch.log(probs.gather(1, action))
        return action, log_prob

class SimpleValueNetwork(nn.Module):
    """Simple value network for testing"""
    
    def __init__(self, state_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        return self.network(x)

class SimpleLearningTester:
    """Tests basic learning mechanics"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Initialize networks
        self.policy_net = SimplePolicyNetwork().to(self.device)
        self.value_net = SimpleValueNetwork().to(self.device)
        
        # Initialize optimizers
        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=0.001)
        
        # Tracking
        self.test_results = {}
        
    def test_gradient_flow(self) -> Dict[str, bool]:
        """Test 1: Verify gradients flow through networks"""
        
        logger.info("üß™ Test 1: Gradient Flow")
        
        results = {
            'policy_gradients_flow': False,
            'value_gradients_flow': False,
            'gradients_nonzero': False,
            'no_nan_gradients': False
        }
        
        # Create dummy batch
        batch_size = 32
        state_dim = 64
        states = torch.randn(batch_size, state_dim).to(self.device)
        
        # Test policy network gradients
        self.policy_optimizer.zero_grad()
        logits = self.policy_net(states)
        policy_loss = -logits.mean()  # Simple loss
        policy_loss.backward()
        
        # Check policy gradients
        policy_grad_norm = 0
        policy_has_gradients = True
        policy_has_nan = False
        
        for param in self.policy_net.parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                policy_grad_norm += grad_norm ** 2
                if torch.isnan(param.grad).any():
                    policy_has_nan = True
            else:
                policy_has_gradients = False
        
        policy_grad_norm = policy_grad_norm ** 0.5
        results['policy_gradients_flow'] = policy_has_gradients
        results['gradients_nonzero'] = policy_grad_norm > 1e-8
        results['no_nan_gradients'] = not policy_has_nan
        
        # Test value network gradients
        self.value_optimizer.zero_grad()
        values = self.value_net(states)
        value_loss = values.mean() ** 2  # Simple MSE-like loss
        value_loss.backward()
        
        # Check value gradients
        value_has_gradients = True
        for param in self.value_net.parameters():
            if param.grad is None:
                value_has_gradients = False
        
        results['value_gradients_flow'] = value_has_gradients
        
        # Log results
        logger.info(f"   Policy gradients flow: {results['policy_gradients_flow']}")
        logger.info(f"   Value gradients flow: {results['value_gradients_flow']}")
        logger.info(f"   Gradients non-zero: {results['gradients_nonzero']} (norm: {policy_grad_norm:.6f})")
        logger.info(f"   No NaN gradients: {results['no_nan_gradients']}")
        
        return results
    
    def test_weight_updates(self) -> Dict[str, bool]:
        """Test 2: Verify weights actually update after optimizer.step()"""
        
        logger.info("üß™ Test 2: Weight Updates")
        
        results = {
            'policy_weights_update': False,
            'value_weights_update': False,
            'weight_changes_significant': False
        }
        
        # Store initial weights
        initial_policy_weights = {}
        initial_value_weights = {}
        
        for name, param in self.policy_net.named_parameters():
            initial_policy_weights[name] = param.clone().detach()
        
        for name, param in self.value_net.named_parameters():
            initial_value_weights[name] = param.clone().detach()
        
        # Perform training step
        batch_size = 32
        states = torch.randn(batch_size, 64).to(self.device)
        
        # Policy update
        self.policy_optimizer.zero_grad()
        logits = self.policy_net(states)
        policy_loss = -logits.mean()
        policy_loss.backward()
        self.policy_optimizer.step()
        
        # Value update
        self.value_optimizer.zero_grad()
        values = self.value_net(states)
        value_loss = values.mean() ** 2
        value_loss.backward()
        self.value_optimizer.step()
        
        # Check weight changes
        total_policy_change = 0
        policy_changed = False
        
        for name, param in self.policy_net.named_parameters():
            change = (param - initial_policy_weights[name]).norm().item()
            total_policy_change += change
            if change > 1e-6:
                policy_changed = True
        
        total_value_change = 0
        value_changed = False
        
        for name, param in self.value_net.named_parameters():
            change = (param - initial_value_weights[name]).norm().item()
            total_value_change += change
            if change > 1e-6:
                value_changed = True
        
        results['policy_weights_update'] = policy_changed
        results['value_weights_update'] = value_changed
        results['weight_changes_significant'] = total_policy_change > 1e-4 and total_value_change > 1e-4
        
        logger.info(f"   Policy weights updated: {results['policy_weights_update']} (change: {total_policy_change:.8f})")
        logger.info(f"   Value weights updated: {results['value_weights_update']} (change: {total_value_change:.8f})")
        logger.info(f"   Changes significant: {results['weight_changes_significant']}")
        
        return results
    
    def test_loss_improvement(self, num_steps: int = 50) -> Dict[str, bool]:
        """Test 3: Verify loss improves over training steps"""
        
        logger.info(f"üß™ Test 3: Loss Improvement ({num_steps} steps)")
        
        results = {
            'policy_loss_decreases': False,
            'value_loss_decreases': False,
            'loss_trend_downward': False
        }
        
        policy_losses = []
        value_losses = []
        
        # Training loop
        for step in range(num_steps):
            batch_size = 32
            states = torch.randn(batch_size, 64).to(self.device)
            
            # Policy training
            self.policy_optimizer.zero_grad()
            logits = self.policy_net(states)
            # Create a learning task: predict target actions
            target_actions = torch.randint(0, 8, (batch_size,)).to(self.device)
            policy_loss = nn.CrossEntropyLoss()(logits, target_actions)
            policy_loss.backward()
            self.policy_optimizer.step()
            policy_losses.append(policy_loss.item())
            
            # Value training
            self.value_optimizer.zero_grad()
            values = self.value_net(states)
            # Create a learning task: predict target values
            target_values = torch.randn(batch_size, 1).to(self.device)
            value_loss = nn.MSELoss()(values, target_values)
            value_loss.backward()
            self.value_optimizer.step()
            value_losses.append(value_loss.item())
        
        # Analyze trends
        early_policy_loss = np.mean(policy_losses[:10])
        late_policy_loss = np.mean(policy_losses[-10:])
        policy_improvement = (early_policy_loss - late_policy_loss) / early_policy_loss
        
        early_value_loss = np.mean(value_losses[:10])
        late_value_loss = np.mean(value_losses[-10:])
        value_improvement = (early_value_loss - late_value_loss) / early_value_loss
        
        results['policy_loss_decreases'] = policy_improvement > 0.05  # 5% improvement
        results['value_loss_decreases'] = value_improvement > 0.05
        results['loss_trend_downward'] = results['policy_loss_decreases'] and results['value_loss_decreases']
        
        logger.info(f"   Policy loss improvement: {policy_improvement:.3%} ({'‚úÖ' if results['policy_loss_decreases'] else '‚ùå'})")
        logger.info(f"   Value loss improvement: {value_improvement:.3%} ({'‚úÖ' if results['value_loss_decreases'] else '‚ùå'})")
        logger.info(f"   Overall trend downward: {results['loss_trend_downward']}")
        
        # Store for plotting
        self.test_results['policy_losses'] = policy_losses
        self.test_results['value_losses'] = value_losses
        
        return results
    
    def test_entropy_changes(self, num_steps: int = 50) -> Dict[str, bool]:
        """Test 4: Verify policy entropy changes appropriately"""
        
        logger.info(f"üß™ Test 4: Entropy Evolution ({num_steps} steps)")
        
        results = {
            'entropy_computed': False,
            'entropy_changes': False,
            'entropy_reasonable': False
        }
        
        entropies = []
        
        # Track entropy over training
        for step in range(num_steps):
            batch_size = 32
            states = torch.randn(batch_size, 64).to(self.device)
            
            with torch.no_grad():
                logits = self.policy_net(states)
                probs = torch.softmax(logits, dim=-1)
                entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
                entropies.append(entropy.item())
            
            # Train the network (with some entropy regularization)
            self.policy_optimizer.zero_grad()
            logits = self.policy_net(states)
            target_actions = torch.randint(0, 8, (batch_size,)).to(self.device)
            policy_loss = nn.CrossEntropyLoss()(logits, target_actions)
            
            # Add entropy regularization to encourage some exploration
            probs = torch.softmax(logits, dim=-1)
            entropy_loss = (probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()
            total_loss = policy_loss + 0.01 * entropy_loss
            
            total_loss.backward()
            self.policy_optimizer.step()
        
        # Analyze entropy
        if entropies:
            results['entropy_computed'] = True
            entropy_std = np.std(entropies)
            results['entropy_changes'] = entropy_std > 0.01  # Entropy should vary
            
            mean_entropy = np.mean(entropies)
            # For 8 actions, max entropy is log(8) ‚âà 2.08
            results['entropy_reasonable'] = 0.1 < mean_entropy < 2.5
        
        logger.info(f"   Entropy computed: {results['entropy_computed']}")
        logger.info(f"   Entropy changes: {results['entropy_changes']}")
        logger.info(f"   Entropy reasonable: {results['entropy_reasonable']}")
        if entropies:
            logger.info(f"   Mean entropy: {np.mean(entropies):.3f}, Std: {np.std(entropies):.3f}")
        
        # Store for plotting
        self.test_results['entropies'] = entropies
        
        return results
    
    def test_memory_efficiency(self) -> Dict[str, bool]:
        """Test 5: Check for memory leaks and efficiency"""
        
        logger.info("üß™ Test 5: Memory Efficiency")
        
        results = {
            'no_memory_leak': False,
            'gradients_cleared': False,
            'reasonable_memory': False
        }
        
        # Measure initial memory
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        # Run many training steps
        for step in range(100):
            batch_size = 32
            states = torch.randn(batch_size, 64).to(self.device)
            
            # Policy update
            self.policy_optimizer.zero_grad()
            logits = self.policy_net(states)
            loss = logits.mean()
            loss.backward()
            self.policy_optimizer.step()
        
        # Measure final memory
        final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        memory_increase = final_memory - initial_memory
        
        # Check if gradients are properly cleared
        gradients_cleared = True
        for param in self.policy_net.parameters():
            if param.grad is not None and param.grad.abs().sum() > 1e-6:
                # Gradients should be zeroed by zero_grad()
                pass
        
        results['no_memory_leak'] = memory_increase < 1024 * 1024  # Less than 1MB increase
        results['gradients_cleared'] = gradients_cleared
        results['reasonable_memory'] = True  # Basic test always passes
        
        logger.info(f"   No memory leak: {results['no_memory_leak']} (increase: {memory_increase/1024:.1f} KB)")
        logger.info(f"   Gradients cleared: {results['gradients_cleared']}")
        
        return results
    
    def run_all_tests(self) -> Dict[str, Dict[str, bool]]:
        """Run all learning verification tests"""
        
        logger.info("üîç RUNNING SIMPLE LEARNING VERIFICATION TESTS")
        logger.info("=" * 60)
        
        all_results = {}
        
        # Run tests
        all_results['gradient_flow'] = self.test_gradient_flow()
        all_results['weight_updates'] = self.test_weight_updates()
        all_results['loss_improvement'] = self.test_loss_improvement()
        all_results['entropy_changes'] = self.test_entropy_changes()
        all_results['memory_efficiency'] = self.test_memory_efficiency()
        
        return all_results
    
    def generate_report(self, results: Dict[str, Dict[str, bool]]) -> str:
        """Generate comprehensive test report"""
        
        report = []
        report.append("=" * 80)
        report.append("üîç SIMPLE LEARNING VERIFICATION TEST REPORT")
        report.append("=" * 80)
        report.append(f"Generated: {torch.datetime.now() if hasattr(torch, 'datetime') else 'now'}")
        report.append(f"Device: {self.device}")
        report.append("")
        
        # Count passes and fails
        total_tests = 0
        passed_tests = 0
        
        for test_name, test_results in results.items():
            test_passed = all(test_results.values())
            total_tests += 1
            if test_passed:
                passed_tests += 1
        
        # Overall status
        if passed_tests == total_tests:
            report.append("üéâ OVERALL STATUS: ALL TESTS PASSED ‚úÖ")
        elif passed_tests >= total_tests * 0.8:
            report.append("üëç OVERALL STATUS: MOSTLY PASSING ‚úÖ")
        else:
            report.append("‚ùå OVERALL STATUS: MULTIPLE FAILURES ‚ùå")
        
        report.append(f"Tests Passed: {passed_tests}/{total_tests}")
        report.append("")
        
        # Detailed results
        report.append("üìã DETAILED TEST RESULTS:")
        report.append("")
        
        for test_name, test_results in results.items():
            test_passed = all(test_results.values())
            status = "‚úÖ PASSED" if test_passed else "‚ùå FAILED"
            
            report.append(f"üîç {test_name.upper()}: {status}")
            
            for check_name, check_passed in test_results.items():
                check_status = "‚úÖ" if check_passed else "‚ùå"
                report.append(f"   {check_status} {check_name}")
            
            report.append("")
        
        # Recommendations
        report.append("üí° RECOMMENDATIONS:")
        
        failed_tests = [name for name, results_dict in results.items() 
                       if not all(results_dict.values())]
        
        if not failed_tests:
            report.append("   ‚úÖ All basic learning mechanics verified!")
            report.append("   ‚úÖ Ready for complex GAELP agent training!")
        else:
            report.append("   ‚ùå Fix failed basic learning tests before proceeding:")
            for test_name in failed_tests:
                if test_name == 'gradient_flow':
                    report.append("      ‚Ä¢ Check network architecture and loss computation")
                elif test_name == 'weight_updates':
                    report.append("      ‚Ä¢ Verify optimizer.step() is called after loss.backward()")
                elif test_name == 'loss_improvement':
                    report.append("      ‚Ä¢ Check learning rate and loss function design")
                elif test_name == 'entropy_changes':
                    report.append("      ‚Ä¢ Verify policy network outputs valid probability distributions")
                elif test_name == 'memory_efficiency':
                    report.append("      ‚Ä¢ Check for memory leaks and proper gradient clearing")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def save_plots(self, save_path: str = "/home/hariravichandran/AELP/simple_learning_plots.png"):
        """Save training plots"""
        
        if not self.test_results:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Simple Learning Verification Plots', fontsize=14)
        
        # Policy loss
        if 'policy_losses' in self.test_results:
            axes[0, 0].plot(self.test_results['policy_losses'], 'b-', alpha=0.7)
            axes[0, 0].set_title('Policy Loss')
            axes[0, 0].set_xlabel('Training Step')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].grid(True, alpha=0.3)
        
        # Value loss
        if 'value_losses' in self.test_results:
            axes[0, 1].plot(self.test_results['value_losses'], 'r-', alpha=0.7)
            axes[0, 1].set_title('Value Loss')
            axes[0, 1].set_xlabel('Training Step')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].grid(True, alpha=0.3)
        
        # Entropy
        if 'entropies' in self.test_results:
            axes[1, 0].plot(self.test_results['entropies'], 'g-', alpha=0.7)
            axes[1, 0].set_title('Policy Entropy')
            axes[1, 0].set_xlabel('Training Step')
            axes[1, 0].set_ylabel('Entropy')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Combined view
        if 'policy_losses' in self.test_results and 'value_losses' in self.test_results:
            axes[1, 1].plot(self.test_results['policy_losses'], 'b-', alpha=0.7, label='Policy')
            axes[1, 1].plot(self.test_results['value_losses'], 'r-', alpha=0.7, label='Value')
            axes[1, 1].set_title('Combined Losses')
            axes[1, 1].set_xlabel('Training Step')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Plots saved to {save_path}")

def main():
    """Run simple learning verification tests"""
    
    print("üß™ SIMPLE LEARNING VERIFICATION TEST SUITE")
    print("=" * 60)
    print("Testing fundamental learning mechanics without complex dependencies")
    print("NO FALLBACKS. NO MOCK LEARNING. REAL VERIFICATION ONLY.")
    print("=" * 60)
    
    # Create tester
    tester = SimpleLearningTester()
    
    # Run all tests
    results = tester.run_all_tests()
    
    # Generate report
    report = tester.generate_report(results)
    print("\n" + report)
    
    # Save plots
    tester.save_plots()
    
    # Save report to file
    report_path = "/home/hariravichandran/AELP/simple_learning_test_report.txt"
    with open(report_path, 'w') as f:
        f.write(report)
    
    print(f"\nüìä Report saved to: {report_path}")
    
    # Final verdict
    total_tests = len(results)
    passed_tests = sum(1 for test_results in results.values() if all(test_results.values()))
    
    if passed_tests == total_tests:
        print("\n‚úÖ ALL BASIC LEARNING TESTS PASSED!")
        print("‚úÖ Fundamental learning mechanics verified!")
        print("‚úÖ Ready for complex GAELP training verification!")
    else:
        print(f"\n‚ùå BASIC LEARNING TESTS FAILED! ({passed_tests}/{total_tests} passed)")
        print("‚ùå Fix basic learning issues before proceeding with GAELP!")
        print("‚ùå Review the test report for specific failure details!")

if __name__ == "__main__":
    main()